{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the EVDS data\n",
    "evds_data = pd.read_excel('EVDS.xlsx')\n",
    "\n",
    "# Display the first few rows of the data \n",
    "evds_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns: Tarih to Date, TP METALIHR G71 to Export Total, TP KREHACBS A1 to Total Domestic loan volume and TP HKFE01 to Housing Price Index\n",
    "evds_data.rename(columns={'Tarih': 'Date', 'TP METALIHR G71': 'Export Total', 'TP KREHACBS A1': 'Total Domestic loan volume', 'TP DK USD A YTL': 'Exchange Rates'}, inplace=True)\n",
    "\n",
    "evds_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to datetime, and the remaining columns to numeric\n",
    "evds_data['Date'] = pd.to_datetime(evds_data['Date'], errors='coerce')\n",
    "\n",
    "# Convert the remaining columns to numeric, coercing errors to NaN\n",
    "evds_data['Export Total'] = pd.to_numeric(evds_data['Export Total'], errors='coerce')\n",
    "evds_data['Total Domestic loan volume'] = pd.to_numeric(evds_data['Total Domestic loan volume'], errors='coerce')\n",
    "evds_data['Exchange Rates'] = pd.to_numeric(evds_data['Exchange Rates'], errors='coerce')\n",
    "\n",
    "# Check the resulting data types\n",
    "evds_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to automate converting the date column to datetime and Search Volume to integer\n",
    "\n",
    "def preprocess_data(df, date_column='Week', search_volume_column='Custom Search Volume', search_volume_new_name='Custom Search Volume'):\n",
    "    \"\"\"\n",
    "    Preprocesses the data by renaming the specified date column to 'Date', \n",
    "    converting it to datetime, and converting the specified search volume \n",
    "    column to numeric with a new specified name.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to preprocess.\n",
    "    - date_column: Name of the column to rename to 'Date' and convert to datetime. Default is 'Week'.\n",
    "    - search_volume_column: Name of the column to convert to numeric. Default is 'Search Volume'.\n",
    "    - search_volume_new_name: New name for the search volume column after conversion. Default is 'Custom Search Volume'.\n",
    "\n",
    "    Returns:\n",
    "    - The preprocessed pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Check if the date column exists in the DataFrame\n",
    "    if date_column in df.columns:\n",
    "        # Rename the date column to 'Date' and convert it to datetime\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "        df.rename(columns={date_column: 'Date'}, inplace=True)\n",
    "    else:\n",
    "        print(f\"Column '{date_column}' not found in the DataFrame.\")\n",
    "\n",
    "    # Check if the search volume column exists in the DataFrame\n",
    "    if search_volume_column in df.columns:\n",
    "        # Convert the search volume column to numeric, coercing errors\n",
    "        df[search_volume_column] = pd.to_numeric(df[search_volume_column], errors='coerce')\n",
    "        # Rename the search volume column to the new specified name\n",
    "        df.rename(columns={search_volume_column: search_volume_new_name}, inplace=True)\n",
    "    else:\n",
    "        print(f\"Column '{search_volume_column}' not found in the DataFrame.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the Google Trends data for \"exports\"\n",
    "exports_data = pd.read_csv('exports.csv')\n",
    "\n",
    "# Display the first few rows of the data \n",
    "exports_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the exports data\n",
    "exports_data = preprocess_data(exports_data, date_column='Week', search_volume_column='Search Volume', search_volume_new_name='Exports Search Volume')\n",
    "\n",
    "exports_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the Google Trends data for \"USD-Lira\"\n",
    "usd_lira_data = pd.read_csv('USD-Lira.csv')\n",
    "\n",
    "# Display the first few rows of the data \n",
    "usd_lira_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the house price data\n",
    "house_price_data = preprocess_data(usd_lira_data, date_column='Week', search_volume_column='Search Volume', search_volume_new_name='USD-Lira Search Volume')\n",
    "\n",
    "house_price_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the Google Trends data for \"loan\"\n",
    "loan_data = pd.read_csv('loan.csv')\n",
    "\n",
    "# Inspection of the first few rows of the data\n",
    "loan_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the loan data\n",
    "loan_data = preprocess_data(loan_data, date_column='Week', search_volume_column='Search Volume', search_volume_new_name='Loan Search Volume')\n",
    "\n",
    "loan_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the interest rate data\n",
    "interest_rate_data = pd.read_csv('interest rates.csv')\n",
    "\n",
    "interest_rate_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_rate_data = preprocess_data(interest_rate_data, date_column='Week', search_volume_column='Search Volume', search_volume_new_name='Interest Rate Search Volume')\n",
    "\n",
    "interest_rate_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the 'Date' column as the index for each DataFrame\n",
    "def set_date_as_index(df):\n",
    "    \"\"\"\n",
    "    Sets the 'Date' column as the index of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to set the 'Date' column as index.\n",
    "\n",
    "    Returns:\n",
    "    - The DataFrame with the 'Date' column as index.\n",
    "    \"\"\"\n",
    "    if 'Date' in df.columns:\n",
    "        df.set_index('Date', inplace=True)\n",
    "    else:\n",
    "        print(\"Column 'Date' not found in the DataFrame.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Set the 'Date' column as the index for each DataFrame\n",
    "evds_data = set_date_as_index(evds_data)\n",
    "exports_data = set_date_as_index(exports_data)\n",
    "usd_lira_data = set_date_as_index(usd_lira_data)\n",
    "loan_data = set_date_as_index(loan_data)\n",
    "interest_rate_data = set_date_as_index(interest_rate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exports_data.head(), loan_data.head(), interest_rate_data.head(), usd_lira_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to aggregate the weekly data to monthly averages\n",
    "def aggregate_weekly_data_to_monthly(df):\n",
    "    \"\"\"\n",
    "    Aggregates the weekly data to monthly averages.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with weekly data to aggregate.\n",
    "\n",
    "    Returns:\n",
    "    - The DataFrame with weekly data aggregated to monthly averages.\n",
    "    \"\"\"\n",
    "    # Resample the weekly data to monthly frequency, taking the mean of each month\n",
    "    df_monthly = df.resample('M').mean()\n",
    "\n",
    "    return df_monthly\n",
    "\n",
    "exports_data = aggregate_weekly_data_to_monthly(exports_data)\n",
    "usd_lira_data = aggregate_weekly_data_to_monthly(usd_lira_data)\n",
    "loan_data = aggregate_weekly_data_to_monthly(loan_data)\n",
    "interest_rate_data = aggregate_weekly_data_to_monthly(interest_rate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exports_data.head(), usd_lira_data.head(), loan_data.head(), interest_rate_data.head(), evds_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evds_data.index = pd.to_datetime(evds_data.index).to_period('M').to_timestamp('M')\n",
    "\n",
    "evds_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the monthly averages of Google Trends data\n",
    "fig, axs = plt.subplots(4, 1, figsize=(14, 18))\n",
    "\n",
    "# Exports\n",
    "axs[0].plot(exports_data.index, exports_data['Exports Search Volume'], marker='o', linestyle='-', color='blue')\n",
    "axs[0].set_title('Monthly Average Search Volume for Exports')\n",
    "axs[0].set_ylabel('Search Volume')\n",
    "\n",
    "# USD-Lira\n",
    "axs[1].plot(usd_lira_data.index, usd_lira_data['USD-Lira Search Volume'], marker='o', linestyle='-', color='green')\n",
    "axs[1].set_title('Monthly Average Search Volume for USD-Lira')\n",
    "axs[1].set_ylabel('Search Volume')\n",
    "\n",
    "# Loan\n",
    "axs[2].plot(loan_data.index, loan_data['Loan Search Volume'], marker='o', linestyle='-', color='red')  # Typo in column name corrected here visually\n",
    "axs[2].set_title('Monthly Average Search Volume for Loan')\n",
    "axs[2].set_ylabel('Search Volume')\n",
    "\n",
    "# Interest Rates\n",
    "axs[3].plot(interest_rate_data.index, interest_rate_data['Interest Rate Search Volume'], marker='o', linestyle='-', color='purple')\n",
    "axs[3].set_title('Monthly Average Search Volume for Interest Rates')\n",
    "axs[3].set_ylabel('Search Volume')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To proceed with correlation analysis, we need to merge the EVDS data with the Google Trends aggregated monthly data\n",
    "\n",
    "# Merge the datasets\n",
    "merged_data = pd.concat([exports_data, usd_lira_data, loan_data, interest_rate_data, evds_data], axis=1)\n",
    "\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values in the merged data\n",
    "\n",
    "merged_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with missing values\n",
    "\n",
    "merged_data = merged_data.dropna()\n",
    "\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = merged_data.corr()\n",
    "\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt = \".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(merged_data.index, merged_data['Export Total'], label='Export Total')\n",
    "plt.plot(merged_data.index, merged_data['Total Domestic loan volume'], label='Total Domestic Loan Volume')\n",
    "plt.plot(merged_data.index, merged_data['Exchange Rates'], label='Exchange Rates')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Time Series Plot of Economic Indicators')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = merged_data['Exchange Rates']\n",
    "X = merged_data[['Exports Search Volume', 'USD-Lira Search Volume', 'Loan Search Volume', 'Interest Rate Search Volume']]\n",
    "\n",
    "split_point = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_point], X[split_point:]\n",
    "Y_train, Y_test = Y[:split_point], Y[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a ARIMA Model \n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "exchange_rates = merged_data['Exchange Rates']\n",
    "\n",
    "# Plot ACF and PACF\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plot_acf(exchange_rates, ax=plt.gca(), lags=10)\n",
    "plt.subplot(122)\n",
    "plot_pacf(exchange_rates, ax=plt.gca(), lags=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search to get ARIMA values\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define the p, d, and q ranges to test\n",
    "p = range(0, 3)  \n",
    "d = range(0, 2)  \n",
    "q = range(0, 3) \n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Grid search\n",
    "best_aic = float('inf')\n",
    "best_order = None\n",
    "best_model = None\n",
    "\n",
    "for param in pdq:\n",
    "    try:\n",
    "        temp_model = ARIMA(exchange_rates, order=param)\n",
    "        results = temp_model.fit()\n",
    "        \n",
    "        # Compare current model's AIC with the best so far\n",
    "        if results.aic < best_aic:\n",
    "            best_aic = results.aic\n",
    "            best_order = param\n",
    "            best_model = results\n",
    "    except:  # Handle cases where the model fails to converge\n",
    "        continue\n",
    "\n",
    "print(f'Best ARIMA{best_order} AIC: {best_aic}')\n",
    "print(best_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation Exports total to stabilize variance\n",
    "import numpy as np\n",
    "\n",
    "merged_data['Log Export Total'] = np.log(merged_data['Export Total'])\n",
    "\n",
    "merged_data[['Export Total', 'Log Export Total']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent and independent variables\n",
    "y_export = merged_data['Log Export Total']\n",
    "X_export = merged_data[['Exports Search Volume', 'USD-Lira Search Volume', 'Loan Search Volume', 'Interest Rate Search Volume']]\n",
    "\n",
    "split_point = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_point], X[split_point:]\n",
    "Y_train, Y_test = Y[:split_point], Y[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a ARIMA Model due to presence of autocorrelation and potential issues with normality of residuals\n",
    "\n",
    "exports_total = merged_data['Log Export Total']\n",
    "\n",
    "# Plot ACF and PACF\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_acf(exports_total, ax=plt.gca(), lags=10)\n",
    "plt.subplot(122)\n",
    "plot_pacf(exports_total, ax=plt.gca(), lags=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search to get ARIMA values\n",
    "\n",
    "# Define the p, d, and q ranges to test\n",
    "p = range(0, 3)  \n",
    "d = range(0, 2)  \n",
    "q = range(0, 3) \n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Grid search\n",
    "best_aic = float('inf')\n",
    "best_order = None\n",
    "best_model = None\n",
    "\n",
    "for param in pdq:\n",
    "    try:\n",
    "        temp_model = ARIMA(exports_total, order=param)\n",
    "        results = temp_model.fit()\n",
    "        \n",
    "        # Compare current model's AIC with the best so far\n",
    "        if results.aic < best_aic:\n",
    "            best_aic = results.aic\n",
    "            best_order = param\n",
    "            best_model = results\n",
    "    except:  # Handle cases where the model fails to converge\n",
    "        continue\n",
    "    \n",
    "print(f\"Best ARIMA{best_order} AIC: {best_aic}\")\n",
    "print(best_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation of the dependent variable\n",
    "merged_data['Log Domestic Loan Volume'] = np.log(merged_data['Total Domestic loan volume'])\n",
    "\n",
    "merged_data[['Total Domestic loan volume', 'Log Domestic Loan Volume']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent and independent variables\n",
    "y_total_loan = merged_data['Log Domestic Loan Volume']\n",
    "X_total_loan = merged_data[['Exports Search Volume', 'USD-Lira Search Volume', 'Loan Search Volume', 'Interest Rate Search Volume']]\n",
    "\n",
    "split_point = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_point], X[split_point:]\n",
    "Y_train, Y_test = Y[:split_point], Y[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Model for Total Domestic Loan Volume\n",
    "\n",
    "total_loan_volume = merged_data['Log Domestic Loan Volume']\n",
    "\n",
    "# Plot ACF and PACF\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_acf(total_loan_volume, ax=plt.gca(), lags=10)\n",
    "plt.subplot(122)\n",
    "plot_pacf(total_loan_volume, ax=plt.gca(), lags=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search to get ARIMA values\n",
    "\n",
    "# Define the p, d, and q ranges to test\n",
    "p = range(0, 3)\n",
    "d = range(0, 2)\n",
    "q = range(0, 3)\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Grid search\n",
    "best_aic = float('inf')\n",
    "best_order = None\n",
    "best_model = None\n",
    "\n",
    "for param in pdq:\n",
    "    try:\n",
    "        temp_model = ARIMA(total_loan_volume, order=param)\n",
    "        results = temp_model.fit()\n",
    "        \n",
    "        # Compare current model's AIC with the best so far\n",
    "        if results.aic < best_aic:\n",
    "            best_aic = results.aic\n",
    "            best_order = param\n",
    "            best_model = results\n",
    "    except:  # Handle cases where the model fails to converge\n",
    "        continue\n",
    "    \n",
    "print(f\"Best ARIMA{best_order} AIC: {best_aic}\")\n",
    "print(best_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
